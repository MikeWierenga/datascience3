{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09d6b886",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "for this weeks assignment i chose to do text clustering. for this assignment i will pick out my own dataset and perform text clustering on it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434e8410",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20cd0203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import PyPDF2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a719b864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /homes/mkwierenga/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /homes/mkwierenga/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /homes/mkwierenga/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /homes/mkwierenga/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a760a72f",
   "metadata": {},
   "source": [
    "# Step 1: load the PDF file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44ece426",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PyPDF2.PdfReader('data/machinelearningbook.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a697f6",
   "metadata": {},
   "source": [
    "## step 2: extract the lines to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8db5095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "chapter = 0\n",
    "for i in range(14, 369):\n",
    "    page = reader.pages[i].extract_text().replace('\\n', ' ').split('.')\n",
    "    for line in page:\n",
    "        if 'CHAPTER' in line:\n",
    "            chapter +=1\n",
    "        line = line.lower()\n",
    "        line = re.sub('\\[.*?\\]', ' ', line)\n",
    "        line = re.sub('[%s]' % re.escape(string.punctuation), ' ', line)\n",
    "        line = re.sub('\\w*\\d\\w*', ' ', line)\n",
    "        line = re.sub('�', ' ', line)\n",
    "        data.append([chapter, line, i])\n",
    "df = pd.DataFrame(data, columns=['chapter', 'line', 'page_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f096a96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun extract and lemmatize function\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text \n",
    "    and pull out only the nouns.'''\n",
    "    # create mask to isolate words that are nouns\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    # store function to split string of words \n",
    "    # into a list of words (tokens)\n",
    "    tokenized = word_tokenize(text)\n",
    "    # store function to lemmatize each word\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    # use list comprehension to lemmatize all words \n",
    "    # and create a list of all nouns\n",
    "    all_nouns = [wordnet_lemmatizer.lemmatize(word) \\\n",
    "    for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    \n",
    "    #return string of joined list of nouns\n",
    "    return ' '.join(all_nouns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4ba4d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chapter introduction machine learning knowledg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>research field intersection statistic intellig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>application machine method year life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>recommendation movie food order product online...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>website facebook amazon netflix part site mach...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                line\n",
       "0  chapter introduction machine learning knowledg...\n",
       "1  research field intersection statistic intellig...\n",
       "2               application machine method year life\n",
       "3  recommendation movie food order product online...\n",
       "4  website facebook amazon netflix part site mach..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_nouns = pd.DataFrame(df[\"line\"].apply(nouns))\n",
    "# Visually Inspect\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7c78524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chapter introduction machine learning knowledg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>research field intersection statistic intellig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>application machine method year life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>recommendation movie food order product online...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>website facebook amazon netflix part site mach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7520</th>\n",
       "      <td>development text processing year scope book re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7521</th>\n",
       "      <td>use vector representation word vector word rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7522</th>\n",
       "      <td>paper “ representation word phrase composi‐ ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7523</th>\n",
       "      <td>introduction subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7524</th>\n",
       "      <td>spacy summary outlook</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7525 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   line\n",
       "0     chapter introduction machine learning knowledg...\n",
       "1     research field intersection statistic intellig...\n",
       "2                  application machine method year life\n",
       "3     recommendation movie food order product online...\n",
       "4     website facebook amazon netflix part site mach...\n",
       "...                                                 ...\n",
       "7520  development text processing year scope book re...\n",
       "7521  use vector representation word vector word rep...\n",
       "7522  paper “ representation word phrase composi‐ ti...\n",
       "7523                               introduction subject\n",
       "7524                              spacy summary outlook\n",
       "\n",
       "[7525 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "534dd7cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'ENGLISH_STOP_WORDS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create a document-term matrix with only nouns\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Store TF-IDF Vectorizer\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# tfidf_text_vectorizer = TfidfVectorizer(stop_words=list(STOP_WORDS))\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tv_noun \u001b[38;5;241m=\u001b[39m TfidfVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mENGLISH_STOP_WORDS\u001b[49m, ngram_range \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), max_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m8\u001b[39m, min_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m01\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Fit and Transform speech noun text to a TF-IDF Doc-Term Matrix\u001b[39;00m\n\u001b[1;32m      6\u001b[0m data_tv_noun \u001b[38;5;241m=\u001b[39m tv_noun\u001b[38;5;241m.\u001b[39mfit_transform(data_nouns\u001b[38;5;241m.\u001b[39mline)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'ENGLISH_STOP_WORDS'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create a document-term matrix with only nouns\n",
    "# Store TF-IDF Vectorizer\n",
    "# tfidf_text_vectorizer = TfidfVectorizer(stop_words=list(STOP_WORDS))\n",
    "tv_noun = TfidfVectorizer(stop_words=text.ENGLISH_STOP_WORDS, ngram_range = (1,1), max_df = .8, min_df = .01)\n",
    "# Fit and Transform speech noun text to a TF-IDF Doc-Term Matrix\n",
    "data_tv_noun = tv_noun.fit_transform(data_nouns.line)\n",
    "# Create data-frame of Doc-Term Matrix with nouns as column names\n",
    "data_dtm_noun = pd.DataFrame(data_tv_noun.toarray(), columns=tv_noun.get_feature_names_out())\n",
    "data_dtm_noun.index = df.index\n",
    "# Visually inspect Document Term Matrix\n",
    "data_dtm_noun.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cc67bc",
   "metadata": {},
   "source": [
    "# Step 2: clean the data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
