{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09d6b886",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "for this weeks assignment i chose to do text clustering. for this assignment i will pick out my own dataset and perform text clustering on it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434e8410",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20cd0203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import PyPDF2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a719b864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /homes/mkwierenga/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /homes/mkwierenga/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /homes/mkwierenga/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /homes/mkwierenga/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a760a72f",
   "metadata": {},
   "source": [
    "# Step 1: load the PDF file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44ece426",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PyPDF2.PdfReader('data/machinelearningbook.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a697f6",
   "metadata": {},
   "source": [
    "## step 2: extract the lines to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8db5095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "chapter = 0\n",
    "for i in range(14, 369):\n",
    "    page = reader.pages[i].extract_text().replace('\\n', ' ').split('.')\n",
    "    for line in page:\n",
    "        if 'CHAPTER' in line:\n",
    "            chapter +=1\n",
    "        line = line.lower()\n",
    "        line = re.sub('\\[.*?\\]', ' ', line)\n",
    "        line = re.sub('[%s]' % re.escape(string.punctuation), ' ', line)\n",
    "        line = re.sub('\\w*\\d\\w*', ' ', line)\n",
    "        line = re.sub('�', ' ', line)\n",
    "        data.append([chapter, line, i])\n",
    "df = pd.DataFrame(data, columns=['chapter', 'line', 'page_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f096a96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun extract and lemmatize function\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text \n",
    "    and pull out only the nouns.'''\n",
    "    # create mask to isolate words that are nouns\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    # store function to split string of words \n",
    "    # into a list of words (tokens)\n",
    "    tokenized = word_tokenize(text)\n",
    "    # store function to lemmatize each word\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    # use list comprehension to lemmatize all words \n",
    "    # and create a list of all nouns\n",
    "    all_nouns = [wordnet_lemmatizer.lemmatize(word) \\\n",
    "    for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    \n",
    "    #return string of joined list of nouns\n",
    "    return ' '.join(all_nouns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4ba4d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chapter introduction machine learning knowledg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>research field intersection statistic intellig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>application machine method year life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>recommendation movie food order product online...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>website facebook amazon netflix part site mach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7520</th>\n",
       "      <td>development text processing year scope book re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7521</th>\n",
       "      <td>use vector representation word vector word rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7522</th>\n",
       "      <td>paper “ representation word phrase composi‐ ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7523</th>\n",
       "      <td>introduction subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7524</th>\n",
       "      <td>spacy summary outlook</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7525 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   line\n",
       "0     chapter introduction machine learning knowledg...\n",
       "1     research field intersection statistic intellig...\n",
       "2                  application machine method year life\n",
       "3     recommendation movie food order product online...\n",
       "4     website facebook amazon netflix part site mach...\n",
       "...                                                 ...\n",
       "7520  development text processing year scope book re...\n",
       "7521  use vector representation word vector word rep...\n",
       "7522  paper “ representation word phrase composi‐ ti...\n",
       "7523                               introduction subject\n",
       "7524                              spacy summary outlook\n",
       "\n",
       "[7525 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_nouns = pd.DataFrame(df[\"line\"].apply(nouns))\n",
    "# Visually Inspect\n",
    "data_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "573fd185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chapter</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>chapter introduction machine learning knowledg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>research field intersection statistic intellig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>application machine method year life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>recommendation movie food order product online...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>website facebook amazon netflix part site mach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7520</th>\n",
       "      <td>7</td>\n",
       "      <td>development text processing year scope book re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7521</th>\n",
       "      <td>7</td>\n",
       "      <td>use vector representation word vector word rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7522</th>\n",
       "      <td>7</td>\n",
       "      <td>paper “ representation word phrase composi‐ ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7523</th>\n",
       "      <td>7</td>\n",
       "      <td>introduction subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7524</th>\n",
       "      <td>7</td>\n",
       "      <td>spacy summary outlook</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7525 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      chapter                                               line\n",
       "0           1  chapter introduction machine learning knowledg...\n",
       "1           1  research field intersection statistic intellig...\n",
       "2           1               application machine method year life\n",
       "3           1  recommendation movie food order product online...\n",
       "4           1  website facebook amazon netflix part site mach...\n",
       "...       ...                                                ...\n",
       "7520        7  development text processing year scope book re...\n",
       "7521        7  use vector representation word vector word rep...\n",
       "7522        7  paper “ representation word phrase composi‐ ti...\n",
       "7523        7                               introduction subject\n",
       "7524        7                              spacy summary outlook\n",
       "\n",
       "[7525 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.merge(df['chapter'], data_nouns, left_index=True, right_index=True)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "534dd7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dtm_matrix(data_nouns):\n",
    "    # Create a document-term matrix with only nouns\n",
    "    # Store TF-IDF Vectorizer\n",
    "    tv_noun = TfidfVectorizer(stop_words=list(text.ENGLISH_STOP_WORDS), ngram_range = (1,1), max_df = .8, min_df = .01)\n",
    "    # Fit and Transform speech noun text to a TF-IDF Doc-Term Matrix\n",
    "    data_tv_noun = tv_noun.fit_transform(data_nouns.line)\n",
    "    # Create data-frame of Doc-Term Matrix with nouns as column names\n",
    "    data_dtm_noun = pd.DataFrame(data_tv_noun.toarray(), columns=tv_noun.get_feature_names_out())\n",
    "#     data_dtm_noun.index = df.index\n",
    "    # Visually inspect Document Term Matrix\n",
    "    return tv_noun, data_dtm_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a5fe3a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, num_top_words, topic_names=None):\n",
    "    '''Given an NMF model, feature_names, and number of top words, print \n",
    "       topic number and its top feature names, up to specified number of top words.'''\n",
    "    # iterate through topics in topic-term matrix, 'H' aka\n",
    "    # model.components_\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        #print topic, topic number, and top words\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i] \\\n",
    "             for i in topic.argsort()[:-num_top_words - 1:-1]]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5ac336e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dtm_noun = [to_dtm_matrix(merged_df[merged_df['chapter'] == 1][['line']]), \n",
    "                 to_dtm_matrix(merged_df[merged_df['chapter'] == 2][['line']]),\n",
    "                 to_dtm_matrix(merged_df[merged_df['chapter'] == 3][['line']]),\n",
    "                 to_dtm_matrix(merged_df[merged_df['chapter'] == 4][['line']]), \n",
    "                 to_dtm_matrix(merged_df[merged_df['chapter'] == 5][['line']]),\n",
    "                 to_dtm_matrix(merged_df[merged_df['chapter'] == 6][['line']]),\n",
    "                 to_dtm_matrix(merged_df[merged_df['chapter'] == 7][['line']])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "720c9c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chapter: 1\n",
      "\n",
      "Topic  0\n",
      "data, array, point, feature, label, training, measurement, flower, iris, output\n",
      "\n",
      "Topic  1\n",
      "scikit, version, learn, import, matplotlib, panda, scipy, ipython, algorithm, tool\n",
      "\n",
      "Topic  2\n",
      "machine, model, learning, application, chapter, dataset, problem, specie, introduction, algorithm\n",
      "\n",
      "Topic  3\n",
      "python, code, package, notebook, library, jupyter, output, tool, scipy, matplotlib\n",
      "\n",
      "Topic  4\n",
      "format, test, train, set, score, print, prediction, shape, knn, model\n",
      "\n",
      "\n",
      "chapter: 2\n",
      "\n",
      "Topic  0\n",
      "model, data, tree, point, parameter, decision, class, regression, prediction, algorithm\n",
      "\n",
      "Topic  1\n",
      "test, score, train, accuracy, set, training, print, fit, ax, prediction\n",
      "\n",
      "Topic  2\n",
      "feature, plt, tree, ax, ylabel, dataset, class, input, cancer, chapter\n",
      "\n",
      "Topic  3\n",
      "format, gbrt, mlp, np, cancer, neighbor, train, ridge, class, state\n",
      "\n",
      "Topic  4\n",
      "figure, plot, mglearn, dataset, ax, decision, layer, boundary, neighbor, result\n",
      "\n",
      "\n",
      "chapter: 3\n",
      "\n",
      "Topic  0\n",
      "cluster, point, center, sample, number, mean, min, label, kmeans, eps\n",
      "\n",
      "Topic  1\n",
      "data, feature, point, algorithm, learning, plot, chapter, representation, dataset, method\n",
      "\n",
      "Topic  2\n",
      "figure, feature, mglearn, plt, example, plot, sklearn, result, dataset, format\n",
      "\n",
      "Topic  3\n",
      "ax, image, shape, test, reshape, transform, train, zip, people, yticks\n",
      "\n",
      "Topic  4\n",
      "component, pca, plt, nmf, image, direction, face, print, imshow, shape\n",
      "\n",
      "\n",
      "chapter: 4\n",
      "\n",
      "Topic  0\n",
      "feature, selection, number, value, input, integer, engineering, workclass, interaction, bin\n",
      "\n",
      "Topic  1\n",
      "plt, plot, figure, regression, output, input, line, date, range, reg\n",
      "\n",
      "Topic  2\n",
      "train, test, score, shape, print, fit, transform, format, poly, interaction\n",
      "\n",
      "Topic  3\n",
      "data, chapter, engineering, way, point, value, column, dummy, machine, training\n",
      "\n",
      "Topic  4\n",
      "model, figure, selection, sklearn, tree, regression, day, transformation, forest, random\n",
      "\n",
      "\n",
      "chapter: 5\n",
      "\n",
      "Topic  0\n",
      "score, test, format, print, set, train, auc, svc, predict, accuracy\n",
      "\n",
      "Topic  1\n",
      "validation, cross, search, split, accuracy, data, score, result, parameter, cv\n",
      "\n",
      "Topic  2\n",
      "gamma, parameter, kernel, svc, grid, search, setting, param, np, figure\n",
      "\n",
      "Topic  3\n",
      "model, data, evaluation, chapter, improvement, parameter, set, metric, training, accuracy\n",
      "\n",
      "Topic  4\n",
      "class, precision, recall, curve, sample, threshold, point, classification, plt, plot\n",
      "\n",
      "\n",
      "chapter: 6\n",
      "\n",
      "Topic  0\n",
      "step, pipeline, estimator, class, transform, sklearn, method, chain, pipe, pca\n",
      "\n",
      "Topic  1\n",
      "format, grid, pipe, param, gridsearchcv, cv, svm, plt, component, random\n",
      "\n",
      "Topic  2\n",
      "score, test, print, train, params, cross, split, val, set, fold\n",
      "\n",
      "Topic  3\n",
      "parameter, model, grid, svc, search, gridsearchcv, logisticregression, param, gamma, selection\n",
      "\n",
      "Topic  4\n",
      "data, validation, train, cross, training, cancer, feature, model, fit, scaler\n",
      "\n",
      "\n",
      "chapter: 7\n",
      "\n",
      "Topic  0\n",
      "word, bag, representation, document, bard, example, number, form, data, count\n",
      "\n",
      "Topic  1\n",
      "feature, number, extraction, idf, token, data, tf, model, stopwords, component\n",
      "\n",
      "Topic  2\n",
      "topic, document, review, lda, movie, model, method, component, modeling, time\n",
      "\n",
      "Topic  3\n",
      "train, data, text, print, chapter, score, validation, cross, test, transform\n",
      "\n",
      "Topic  4\n",
      "format, grid, cv, np, param, logisticregression, vect, search, class, train\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chapter = 1\n",
    "for data in data_dtm_noun: \n",
    "    nmf_model = NMF(5)\n",
    "    # Learn an NMF model for given Document Term Matrix 'V' \n",
    "    # Extract the document-topic matrix 'W'\n",
    "    doc_topic = nmf_model.fit_transform(data[1])\n",
    "    # Extract top words from the topic-term matrix 'H' \n",
    "    doc_topic\n",
    "    print(f'chapter: {chapter}')\n",
    "    display_topics(nmf_model, data[0].get_feature_names_out(), 10)\n",
    "    print('\\n')\n",
    "    chapter +=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
