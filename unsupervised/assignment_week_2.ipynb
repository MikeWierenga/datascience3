{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09d6b886",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "for this weeks assignment i chose to do text clustering. for this assignment i will pick out my own dataset and perform text clustering on it.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "434e8410",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "20cd0203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import PyPDF2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a719b864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Mike\n",
      "[nltk_data]     Wierenga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Mike Wierenga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Mike\n",
      "[nltk_data]     Wierenga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Mike\n",
      "[nltk_data]     Wierenga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a760a72f",
   "metadata": {},
   "source": [
    "# Step 1: load the PDF file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ce5d4651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    with open(\"../config.yaml\", 'r') as stream:\n",
    "        config = yaml.safe_load(stream)\n",
    "    return config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b572322",
   "metadata": {},
   "source": [
    "For this assignment i downloaded a book from the internet about machine learning. This is a PDF file so I installed PyPDF2 which has easy to use functions to read all the pages in the file.\n",
    "documentation PyPDF2: https://pypdf2.readthedocs.io/en/latest/modules/PdfReader.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "44ece426",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()\n",
    "reader = PyPDF2.PdfReader(config['dataclusteringfile'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4a697f6",
   "metadata": {},
   "source": [
    "## step 2: extract the lines to dataframe\n",
    "In the code below i looped through all the pages to individually. <br>\n",
    "With a short scan I found out that each chapter begins with the word CHAPTER all capital laters. <br> \n",
    "I wanted to check on this because i wanted use text clustering for each chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8db5095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PDF_to_dataframe(start_page, end_page, pdfreader, chapter_word):\n",
    "    \"\"\"\n",
    "    Function to go from PDF to a dataframe with sentences\n",
    "    chapters can be split via a specific chapter word\n",
    "    params: startpage(int), end_page(int), pdfreader(PyPDF2.PdfReader) chapter_word(string)\n",
    "    returns: dataframe object\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    chapter = 0\n",
    "    for i in range(start_page, end_page):\n",
    "        page = pdfreader.pages[i].extract_text().replace('\\n', ' ').split('.')\n",
    "        for line in page:\n",
    "            if chapter_word in line:\n",
    "                chapter +=1\n",
    "            line = line.lower()\n",
    "            line = re.sub('\\[.*?\\]', ' ', line)\n",
    "            line = re.sub('[%s]' % re.escape(string.punctuation), ' ', line)\n",
    "            line = re.sub('\\w*\\d\\w*', ' ', line)\n",
    "            line = re.sub('�', ' ', line)\n",
    "            data.append([chapter, line, i])\n",
    "    df = pd.DataFrame(data, columns=['chapter', 'line', 'page_number'])\n",
    "    return df\n",
    "df = PDF_to_dataframe(start_page=14, end_page=369, pdfreader=reader, chapter_word='CHAPTER')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e5233a7",
   "metadata": {},
   "source": [
    "Here extracted all the nouns which are all words that represents a person, thing, concept or place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f096a96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun extract and lemmatize function\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text \n",
    "    and pull out only the nouns.'''\n",
    "    # create mask to isolate words that are nouns\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    # store function to split string of words \n",
    "    # into a list of words (tokens)\n",
    "    tokenized = word_tokenize(text)\n",
    "    # store function to lemmatize each word\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    # use list comprehension to lemmatize all words \n",
    "    # and create a list of all nouns\n",
    "    all_nouns = [wordnet_lemmatizer.lemmatize(word) \\\n",
    "    for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    \n",
    "    #return string of joined list of nouns\n",
    "    return ' '.join(all_nouns)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9aa68d44",
   "metadata": {},
   "source": [
    "Below is the dataframe without the chapter number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c4ba4d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chapter introduction machine learning knowledg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>research field intersection statistic intellig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>application machine method year life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>recommendation movie food order product online...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>website facebook amazon netflix part site mach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7520</th>\n",
       "      <td>development text processing year scope book re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7521</th>\n",
       "      <td>use vector representation word vector word rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7522</th>\n",
       "      <td>paper “ representation word phrase composi‐ ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7523</th>\n",
       "      <td>introduction subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7524</th>\n",
       "      <td>spacy summary outlook</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7525 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   line\n",
       "0     chapter introduction machine learning knowledg...\n",
       "1     research field intersection statistic intellig...\n",
       "2                  application machine method year life\n",
       "3     recommendation movie food order product online...\n",
       "4     website facebook amazon netflix part site mach...\n",
       "...                                                 ...\n",
       "7520  development text processing year scope book re...\n",
       "7521  use vector representation word vector word rep...\n",
       "7522  paper “ representation word phrase composi‐ ti...\n",
       "7523                               introduction subject\n",
       "7524                              spacy summary outlook\n",
       "\n",
       "[7525 rows x 1 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_nouns = pd.DataFrame(df[\"line\"].apply(nouns))\n",
    "data_nouns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84f6ba70",
   "metadata": {},
   "source": [
    "Chapter numbers have been added "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "573fd185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chapter</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>chapter introduction machine learning knowledg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>research field intersection statistic intellig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>application machine method year life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>recommendation movie food order product online...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>website facebook amazon netflix part site mach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7520</th>\n",
       "      <td>7</td>\n",
       "      <td>development text processing year scope book re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7521</th>\n",
       "      <td>7</td>\n",
       "      <td>use vector representation word vector word rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7522</th>\n",
       "      <td>7</td>\n",
       "      <td>paper “ representation word phrase composi‐ ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7523</th>\n",
       "      <td>7</td>\n",
       "      <td>introduction subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7524</th>\n",
       "      <td>7</td>\n",
       "      <td>spacy summary outlook</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7525 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      chapter                                               line\n",
       "0           1  chapter introduction machine learning knowledg...\n",
       "1           1  research field intersection statistic intellig...\n",
       "2           1               application machine method year life\n",
       "3           1  recommendation movie food order product online...\n",
       "4           1  website facebook amazon netflix part site mach...\n",
       "...       ...                                                ...\n",
       "7520        7  development text processing year scope book re...\n",
       "7521        7  use vector representation word vector word rep...\n",
       "7522        7  paper “ representation word phrase composi‐ ti...\n",
       "7523        7                               introduction subject\n",
       "7524        7                              spacy summary outlook\n",
       "\n",
       "[7525 rows x 2 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.merge(df['chapter'], data_nouns, left_index=True, right_index=True)\n",
    "merged_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef89e6ee",
   "metadata": {},
   "source": [
    "A model doesn't work with words It does with numbers so we need a way to get from the words of the pages into frequencies of how often a wordt has been used. A document-term matrix is a mathmatical matrix that describes the frequency of how often words occur.\n",
    "\n",
    "for example i have a document with the text: 'Here we will use NMF, NMF is a method ....' <br>\n",
    "Most of these words would have a count of 1 except for NMF which would have 2.\n",
    "Reference: https://en.wikipedia.org/wiki/Document-term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "534dd7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dtm_matrix(data_nouns):\n",
    "    # Create a document-term matrix with only nouns\n",
    "    # Store TF-IDF Vectorizer\n",
    "    tv_noun = TfidfVectorizer(stop_words=list(text.ENGLISH_STOP_WORDS), ngram_range = (1,1), max_df = .8, min_df = .01)\n",
    "    # Fit and Transform speech noun text to a TF-IDF Doc-Term Matrix\n",
    "    data_tv_noun = tv_noun.fit_transform(data_nouns.line)\n",
    "    # Create data-frame of Doc-Term Matrix with nouns as column names\n",
    "    data_dtm_noun = pd.DataFrame(data_tv_noun.toarray(), columns=tv_noun.get_feature_names_out())\n",
    "#     data_dtm_noun.index = df.index\n",
    "    # Visually inspect Document Term Matrix\n",
    "    return tv_noun, data_dtm_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a5fe3a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, num_top_words, topic_names=None):\n",
    "    '''Given an NMF model, feature_names, and number of top words, print \n",
    "       topic number and its top feature names, up to specified number of top words.'''\n",
    "    # iterate through topics in topic-term matrix, 'H' aka\n",
    "    # model.components_\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        #print topic, topic number, and top words\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i] \\\n",
    "             for i in topic.argsort()[:-num_top_words - 1:-1]]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5ac336e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dtm_noun = [to_dtm_matrix(merged_df[merged_df['chapter'] == 1][['line']]), \n",
    "                 to_dtm_matrix(merged_df[merged_df['chapter'] == 2][['line']]),\n",
    "                 to_dtm_matrix(merged_df[merged_df['chapter'] == 3][['line']]),\n",
    "                 to_dtm_matrix(merged_df[merged_df['chapter'] == 4][['line']]), \n",
    "                 to_dtm_matrix(merged_df[merged_df['chapter'] == 5][['line']]),\n",
    "                 to_dtm_matrix(merged_df[merged_df['chapter'] == 6][['line']]),\n",
    "                 to_dtm_matrix(merged_df[merged_df['chapter'] == 7][['line']])]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a845a327",
   "metadata": {},
   "source": [
    "# Apply Non-negative matrix factorization(NMF)\n",
    "NMF is used to decompose the document term matrix into two smaller matrices: \n",
    "    - the document-topic matrix\n",
    "    - topic term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "720c9c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mike Wierenga\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n",
      "c:\\Users\\Mike Wierenga\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n",
      "c:\\Users\\Mike Wierenga\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n",
      "c:\\Users\\Mike Wierenga\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n",
      "c:\\Users\\Mike Wierenga\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chapter: 1\n",
      "\n",
      "Topic  0\n",
      "data, array, point, feature, label, training, measurement, flower, iris, output\n",
      "\n",
      "Topic  1\n",
      "scikit, version, learn, import, matplotlib, panda, scipy, ipython, algorithm, tool\n",
      "\n",
      "Topic  2\n",
      "machine, model, learning, application, chapter, dataset, problem, specie, introduction, algorithm\n",
      "\n",
      "Topic  3\n",
      "python, code, package, notebook, library, jupyter, output, tool, scipy, matplotlib\n",
      "\n",
      "Topic  4\n",
      "format, test, train, set, score, print, prediction, shape, knn, model\n",
      "\n",
      "\n",
      "chapter: 2\n",
      "\n",
      "Topic  0\n",
      "model, data, tree, point, parameter, decision, class, regression, prediction, algorithm\n",
      "\n",
      "Topic  1\n",
      "test, score, train, accuracy, set, training, print, fit, ax, prediction\n",
      "\n",
      "Topic  2\n",
      "feature, plt, tree, ax, ylabel, dataset, class, input, cancer, chapter\n",
      "\n",
      "Topic  3\n",
      "format, gbrt, mlp, np, cancer, neighbor, train, ridge, class, state\n",
      "\n",
      "Topic  4\n",
      "figure, plot, mglearn, dataset, ax, decision, layer, boundary, neighbor, result\n",
      "\n",
      "\n",
      "chapter: 3\n",
      "\n",
      "Topic  0\n",
      "cluster, point, center, sample, number, mean, min, label, kmeans, eps\n",
      "\n",
      "Topic  1\n",
      "data, feature, point, algorithm, learning, plot, chapter, representation, dataset, method\n",
      "\n",
      "Topic  2\n",
      "figure, feature, mglearn, plt, example, plot, sklearn, result, dataset, format\n",
      "\n",
      "Topic  3\n",
      "ax, image, shape, test, reshape, transform, train, zip, people, yticks\n",
      "\n",
      "Topic  4\n",
      "component, pca, plt, nmf, image, direction, face, print, imshow, shape\n",
      "\n",
      "\n",
      "chapter: 4\n",
      "\n",
      "Topic  0\n",
      "feature, selection, number, value, input, integer, engineering, workclass, interaction, bin\n",
      "\n",
      "Topic  1\n",
      "plt, plot, figure, regression, output, input, line, date, range, reg\n",
      "\n",
      "Topic  2\n",
      "train, test, score, shape, print, fit, transform, format, poly, interaction\n",
      "\n",
      "Topic  3\n",
      "data, chapter, engineering, way, point, value, column, dummy, machine, training\n",
      "\n",
      "Topic  4\n",
      "model, figure, selection, sklearn, tree, regression, day, transformation, forest, random\n",
      "\n",
      "\n",
      "chapter: 5\n",
      "\n",
      "Topic  0\n",
      "score, test, format, print, set, train, auc, svc, predict, accuracy\n",
      "\n",
      "Topic  1\n",
      "validation, cross, search, split, accuracy, data, score, result, parameter, cv\n",
      "\n",
      "Topic  2\n",
      "gamma, parameter, kernel, svc, grid, search, setting, param, np, figure\n",
      "\n",
      "Topic  3\n",
      "model, data, evaluation, chapter, improvement, parameter, set, metric, training, accuracy\n",
      "\n",
      "Topic  4\n",
      "class, precision, recall, curve, sample, threshold, point, classification, plt, plot\n",
      "\n",
      "\n",
      "chapter: 6\n",
      "\n",
      "Topic  0\n",
      "step, pipeline, estimator, class, transform, sklearn, method, chain, pipe, pca\n",
      "\n",
      "Topic  1\n",
      "format, grid, pipe, param, gridsearchcv, cv, svm, plt, component, random\n",
      "\n",
      "Topic  2\n",
      "score, test, print, train, params, cross, split, val, set, fold\n",
      "\n",
      "Topic  3\n",
      "parameter, model, grid, svc, search, gridsearchcv, logisticregression, param, gamma, selection\n",
      "\n",
      "Topic  4\n",
      "data, validation, train, cross, training, cancer, feature, model, fit, scaler\n",
      "\n",
      "\n",
      "chapter: 7\n",
      "\n",
      "Topic  0\n",
      "word, bag, representation, document, bard, example, number, form, data, count\n",
      "\n",
      "Topic  1\n",
      "feature, number, extraction, idf, token, data, tf, model, stopwords, component\n",
      "\n",
      "Topic  2\n",
      "topic, document, review, lda, movie, model, method, component, modeling, time\n",
      "\n",
      "Topic  3\n",
      "train, data, text, print, chapter, score, validation, cross, test, transform\n",
      "\n",
      "Topic  4\n",
      "format, grid, cv, np, param, logisticregression, vect, search, class, train\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mike Wierenga\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n",
      "c:\\Users\\Mike Wierenga\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "chapter = 1\n",
    "for data in data_dtm_noun: \n",
    "    nmf_model = NMF(5)\n",
    "    # Learn an NMF model for given Document Term Matrix 'V' \n",
    "    # Extract the document-topic matrix 'W'\n",
    "    doc_topic = nmf_model.fit_transform(data[1])\n",
    "    # Extract top words from the topic-term matrix 'H' \n",
    "    doc_topic\n",
    "    print(f'chapter: {chapter}')\n",
    "    display_topics(nmf_model, data[0].get_feature_names_out(), 10)\n",
    "    print('\\n')\n",
    "    chapter +=1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44c41689",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "Looking at the results without reading a word from the PDF i could allready get an idea of what the book is about.<br>\n",
    "For example in chapter 1 the common iris dataset has been used with knn. <br>\n",
    "There are still some words I would remove like the words: chapter, import, ipython"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
